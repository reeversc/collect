{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLtPCXcIw-I1"
   },
   "outputs": [],
   "source": [
    "!pip install -U langchain langchain-openai openai supabase PyPDF2 tiktoken -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHCbieILyLgs"
   },
   "outputs": [],
   "source": [
    "!pip install openai pypdf2 langchain supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XsVcW3dSxCT3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"SUPABASE_URL\"] = \"https://udesnqnzdqidtkujyubo.supabase.co\"\n",
    "os.environ[\"SUPABASE_KEY\"] = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InVkZXNucW56ZHFpZHRrdWp5dWJvIiwicm9sZSI6ImFub24iLCJpYXQiOjE3MjYyODkxMTIsImV4cCI6MjA0MTg2NTExMn0.wVuqkkrTA1OUcpvEz_SUDj4N32wxIUGooUKH-EKl2gU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3YzqaMkHv67"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from openai.error import OpenAIError\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import SupabaseVectorStore\n",
    "from supabase import create_client, Client\n",
    "from supabase.lib.client_options import ClientOptions\n",
    "from google.colab import userdata\n",
    "\n",
    "# Set up OpenAI client\n",
    "openai_client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
    "\n",
    "# Set up Supabase client\n",
    "supabase_url = userdata.get('SUPABASE_URL')\n",
    "supabase_key = userdata.get('SUPABASE_KEY')\n",
    "supabase: Client = create_client(supabase_url, supabase_key, options=ClientOptions(timeout=60))\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf = PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def split_text_into_chunks(text, chunk_size=500, chunk_overlap=50):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "    )\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "def summarize_chunk(chunk):\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",  # Using gpt-3.5-turbo for faster processing and lower cost\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert summarizer. Your task is to create a concise, informative summary of the given text. Capture the main ideas and key points in a single sentence.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Please summarize the following text:\\n\\n{chunk}\"}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def rate_limited_summarize(chunks, max_requests_per_minute=60):\n",
    "    summaries = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if i > 0 and i % max_requests_per_minute == 0:\n",
    "            time.sleep(60)  # Wait for a minute after every max_requests_per_minute requests\n",
    "        summaries.append(summarize_chunk(chunk))\n",
    "    return summaries\n",
    "\n",
    "def store_in_supabase(chunks, summaries):\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=userdata.get('OPENAI_API_KEY'))\n",
    "    metadatas = [{\"source\": f\"chunk_{i}\", \"original_text\": chunk} for i, chunk in enumerate(chunks)]\n",
    "    vector_store = SupabaseVectorStore.from_texts(\n",
    "        texts=summaries,\n",
    "        embedding=embeddings,\n",
    "        metadatas=metadatas,\n",
    "        client=supabase,\n",
    "        table_name=\"document_summaries\",\n",
    "    )\n",
    "    return vector_store\n",
    "\n",
    "def main(pdf_path, chunk_size=500, chunk_overlap=50):\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f\"The file {pdf_path} does not exist.\")\n",
    "\n",
    "    # Extract text from PDF\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    # Split text into chunks\n",
    "    chunks = split_text_into_chunks(text, chunk_size, chunk_overlap)\n",
    "\n",
    "    # Summarize each chunk with rate limiting\n",
    "    summaries = rate_limited_summarize(chunks)\n",
    "\n",
    "    # Store summaries in Supabase\n",
    "    vector_store = store_in_supabase(chunks, summaries)\n",
    "\n",
    "    print(f\"Processed {len(chunks)} chunks and stored their summaries in Supabase.\")\n",
    "    return summaries\n",
    "\n",
    "# Specify the full path to your PDF file\n",
    "pdf_path = \"/content/thiamine.pdf\"  # Replace with your actual PDF filename\n",
    "\n",
    "# Run the main function\n",
    "try:\n",
    "    summaries = main(pdf_path, chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "    # Print summaries\n",
    "    for i, summary in enumerate(summaries, 1):\n",
    "        print(f\"Chunk {i} summary: {summary}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please make sure the file exists in the specified location.\")\n",
    "except OpenAIError as e:\n",
    "    print(f\"OpenAI API error: {e}\")\n",
    "    print(\"Please check your OpenAI API key and quota.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "    print(\"Please check your API keys and ensure all dependencies are correctly installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQVQtHpyQ8Mp"
   },
   "outputs": [],
   "source": [
    "#radm\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import SupabaseVectorStore\n",
    "from supabase import create_client, Client\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up OpenAI client\n",
    "openai_client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# Set up Supabase client\n",
    "supabase_url = os.environ[\"SUPABASE_URL\"]\n",
    "supabase_key = os.environ[\"SUPABASE_KEY\"]\n",
    "supabase: Client = create_client(supabase_url, supabase_key)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf = PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def split_text_into_chunks(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "    )\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "def summarize_chunk(chunk):\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Summarize the following text in one sentence:\"},\n",
    "            {\"role\": \"user\", \"content\": chunk}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def store_in_supabase(chunks, summaries):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    metadatas = [{\"source\": f\"chunk_{i}\", \"original_text\": chunk} for i, chunk in enumerate(chunks)]\n",
    "    vector_store = SupabaseVectorStore.from_texts(\n",
    "        texts=summaries,\n",
    "        embedding=embeddings,\n",
    "        metadatas=metadatas,\n",
    "        client=supabase,\n",
    "        table_name=\"document_summaries\",\n",
    "    )\n",
    "    return vector_store\n",
    "\n",
    "def contextual_analysis(chunk, summary, other_summaries):\n",
    "    context = \"\\n\".join([f\"Chunk {i+1}: {s}\" for i, s in enumerate(other_summaries)])\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following chunk of information in the context of the other summaries provided.\n",
    "    Decide how to think about or consider this information based on its relationship to the other chunks.\n",
    "\n",
    "    Current Chunk: {chunk}\n",
    "\n",
    "    Current Chunk Summary: {summary}\n",
    "\n",
    "    Summaries of Other Chunks:\n",
    "    {context}\n",
    "\n",
    "    Please provide:\n",
    "    1. A decision on how to frame or consider this chunk of information.\n",
    "    2. An explanation of why this framing is appropriate given the context.\n",
    "    3. Potential implications or insights that arise from considering this chunk in this way.\n",
    "    4. Connections: List the numbers of other chunks (if any) that are strongly related to this chunk, and briefly explain the connection.\n",
    "\n",
    "    Format your response as follows:\n",
    "    Framing Decision: [How to think about or consider this chunk]\n",
    "    Rationale: [Explanation of why this framing is appropriate]\n",
    "    Implications: [Potential insights or consequences of this framing]\n",
    "    Connections: [List of connected chunk numbers and brief explanations]\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an analytical system deciding how to frame and consider information in context.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def identify_connections(analyses):\n",
    "    connections = []\n",
    "    for i, analysis in enumerate(analyses):\n",
    "        chunk_connections = []\n",
    "        connection_section = analysis.split(\"Connections:\")[-1].strip()\n",
    "        for j in range(len(analyses)):\n",
    "            if i != j and str(j+1) in connection_section:\n",
    "                chunk_connections.append(j)\n",
    "        connections.append(chunk_connections)\n",
    "    return connections\n",
    "\n",
    "def visualize_connections(connections):\n",
    "    G = nx.Graph()\n",
    "    for i, connected_chunks in enumerate(connections):\n",
    "        G.add_node(i)\n",
    "        for j in connected_chunks:\n",
    "            G.add_edge(i, j)\n",
    "    pos = nx.spring_layout(G)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=500, font_size=10, font_weight='bold')\n",
    "    plt.title(\"Chunk Connections\")\n",
    "    plt.savefig(\"chunk_connections.png\")\n",
    "    plt.close()\n",
    "\n",
    "def generate_overall_insights(summaries, analyses, connections):\n",
    "    context = \"\\n\".join([f\"Chunk {i+1} Summary: {summary}\" for i, summary in enumerate(summaries)])\n",
    "    analyses_text = \"\\n\\n\".join([f\"Analysis of Chunk {i+1}:\\n{analysis}\" for i, analysis in enumerate(analyses)])\n",
    "    connections_text = \"\\n\".join([f\"Chunk {i+1} is connected to chunks: {', '.join(map(str, [c+1 for c in connected]))}\" for i, connected in enumerate(connections)])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Based on the following summaries, analyses, and connections of chunks from a document, generate overall insights about the entire document.\n",
    "\n",
    "    Summaries:\n",
    "    {context}\n",
    "\n",
    "    Analyses:\n",
    "    {analyses_text}\n",
    "\n",
    "    Connections:\n",
    "    {connections_text}\n",
    "\n",
    "    Please provide:\n",
    "    1. 3-5 key themes or ideas that emerge from the document as a whole\n",
    "    2. Any significant patterns, trends, or relationships you notice across the chunks\n",
    "    3. Overall implications or conclusions that can be drawn from the document\n",
    "    4. Any areas of contradiction or tension within the document\n",
    "    5. Suggestions for further investigation or analysis based on these insights\n",
    "\n",
    "    Format your response clearly with headings for each of these sections.\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an analytical system providing overall insights on a document based on detailed chunk analyses.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def main(pdf_path):\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f\"The file {pdf_path} does not exist.\")\n",
    "\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    print(\"Splitting text into chunks...\")\n",
    "    chunks = split_text_into_chunks(text)\n",
    "\n",
    "    print(\"Summarizing chunks...\")\n",
    "    summaries = [summarize_chunk(chunk) for chunk in chunks]\n",
    "\n",
    "    print(\"Storing summaries in Supabase...\")\n",
    "    vector_store = store_in_supabase(chunks, summaries)\n",
    "\n",
    "    print(\"Performing contextual analysis on chunks...\")\n",
    "    analyses = []\n",
    "    for i, (chunk, summary) in enumerate(zip(chunks, summaries)):\n",
    "        other_summaries = summaries[:i] + summaries[i+1:]\n",
    "        analysis = contextual_analysis(chunk, summary, other_summaries)\n",
    "        analyses.append(analysis)\n",
    "\n",
    "    print(\"Identifying connections between chunks...\")\n",
    "    connections = identify_connections(analyses)\n",
    "\n",
    "    print(\"Visualizing chunk connections...\")\n",
    "    visualize_connections(connections)\n",
    "\n",
    "    print(\"Generating overall document insights...\")\n",
    "    overall_insights = generate_overall_insights(summaries, analyses, connections)\n",
    "\n",
    "    print(f\"Processed {len(chunks)} chunks, performed contextual analyses, identified connections, and generated overall insights.\")\n",
    "    return summaries, analyses, connections, overall_insights\n",
    "\n",
    "# Specify the full path to your PDF file\n",
    "pdf_path = \"/content/thiamine.pdf\"  # Replace with your actual PDF filename\n",
    "\n",
    "try:\n",
    "    summaries, analyses, connections, overall_insights = main(pdf_path)\n",
    "\n",
    "    print(\"\\nChunk Summaries and Analyses:\")\n",
    "    for i, (summary, analysis) in enumerate(zip(summaries, analyses), 1):\n",
    "        print(f\"\\nChunk {i} summary: {summary}\")\n",
    "        print(f\"Contextual Analysis for chunk {i}:\")\n",
    "        print(analysis)\n",
    "        print(f\"Connected to chunks: {[c+1 for c in connections[i-1]]}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    print(\"\\nOverall Document Insights:\")\n",
    "    print(overall_insights)\n",
    "    print(\"\\nChunk connection visualization saved as 'chunk_connections.png'\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please make sure the file exists in the specified location.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(\"Please check your API keys and ensure all dependencies are correctly installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMCgbhS5dBwL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import SupabaseVectorStore\n",
    "from supabase import create_client, Client\n",
    "from neo4j import GraphDatabase\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Set up OpenAI client\n",
    "openai_client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# Set up Supabase client\n",
    "supabase_url = os.environ[\"SUPABASE_URL\"]\n",
    "supabase_key = os.environ[\"SUPABASE_KEY\"]\n",
    "supabase: Client = create_client(supabase_url, supabase_key)\n",
    "\n",
    "# Set up Neo4j client\n",
    "neo4j_uri = os.environ[\"NEO4J_URI\"]\n",
    "neo4j_user = os.environ[\"NEO4J_USER\"]\n",
    "neo4j_password = os.environ[\"NEO4J_PASSWORD\"]\n",
    "neo4j_driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf = PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def split_text_into_chunks(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "    )\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "def process_chunks(chunks):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    chunk_embeddings = embeddings.embed_documents(chunks)\n",
    "    return chunk_embeddings\n",
    "\n",
    "def analyze_chunk_relationships(chunk_embeddings):\n",
    "    similarity_matrix = cosine_similarity(chunk_embeddings)\n",
    "    return similarity_matrix\n",
    "\n",
    "def identify_key_concepts(chunks):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(chunks)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    key_concepts = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        tfidf_row = tfidf_matrix[i].toarray()[0]\n",
    "        sorted_indices = tfidf_row.argsort()[::-1]\n",
    "        top_terms = [feature_names[idx] for idx in sorted_indices[:5]]\n",
    "        key_concepts.append(top_terms)\n",
    "\n",
    "    return key_concepts\n",
    "\n",
    "def store_in_neo4j(chunks, similarity_matrix, key_concepts):\n",
    "    with neo4j_driver.session() as session:\n",
    "        # Clear existing data\n",
    "        session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "\n",
    "        # Create nodes for chunks\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            session.run(\n",
    "                \"CREATE (c:Chunk {id: $id, content: $content})\",\n",
    "                id=i, content=chunk[:200]  # Storing first 200 characters as preview\n",
    "            )\n",
    "\n",
    "        # Create relationships based on similarity\n",
    "        for i in range(len(similarity_matrix)):\n",
    "            for j in range(i+1, len(similarity_matrix)):\n",
    "                if similarity_matrix[i][j] > 0.5:  # Adjust threshold as needed\n",
    "                    session.run(\n",
    "                        \"MATCH (c1:Chunk {id: $id1}), (c2:Chunk {id: $id2}) \"\n",
    "                        \"CREATE (c1)-[:SIMILAR {weight: $weight}]->(c2)\",\n",
    "                        id1=i, id2=j, weight=float(similarity_matrix[i][j])\n",
    "                    )\n",
    "\n",
    "        # Create nodes for key concepts and link to chunks\n",
    "        for i, concepts in enumerate(key_concepts):\n",
    "            for concept in concepts:\n",
    "                session.run(\n",
    "                    \"MATCH (c:Chunk {id: $chunk_id}) \"\n",
    "                    \"MERGE (k:Concept {name: $concept}) \"\n",
    "                    \"CREATE (c)-[:HAS_CONCEPT]->(k)\",\n",
    "                    chunk_id=i, concept=concept\n",
    "                )\n",
    "\n",
    "def generate_emergent_ideas(chunks, similarity_matrix, key_concepts):\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following information about a document's chunks and their relationships:\n",
    "\n",
    "    1. Number of chunks: {len(chunks)}\n",
    "    2. Similarity matrix:\n",
    "       {similarity_matrix.tolist()}\n",
    "    3. Key concepts per chunk:\n",
    "       {key_concepts}\n",
    "\n",
    "    Based on this information, please generate ideas using the following structured approach:\n",
    "\n",
    "    1. Information Gathering:\n",
    "       - Identify the most significant concepts and their relationships from the provided data.\n",
    "       - Research and list any additional relevant information needed to fully understand these concepts.\n",
    "\n",
    "    2. Comprehension of Scientific Principles:\n",
    "       - Explain the fundamental scientific principles underlying the key concepts.\n",
    "       - Identify any interdisciplinary connections between different areas of knowledge.\n",
    "\n",
    "    3. Logical Synthesis:\n",
    "       - Connect the key concepts to form new ideas or hypotheses.\n",
    "       - Use cause-and-effect reasoning to explore potential outcomes of these connections.\n",
    "\n",
    "    4. Critical Analysis:\n",
    "       - Evaluate the potential risks and benefits of the synthesized ideas.\n",
    "       - Consider alternative approaches or perspectives.\n",
    "\n",
    "    5. Application of Knowledge:\n",
    "       - Formulate practical strategies or solutions based on the synthesized ideas.\n",
    "       - Suggest ways to implement these strategies in real-world contexts.\n",
    "\n",
    "    6. Reflection and Confirmation:\n",
    "       - Propose methods to corroborate or test the generated ideas.\n",
    "       - Suggest ways to monitor outcomes and gather evidence of effectiveness.\n",
    "\n",
    "    7. Demonstration of Domain Literacy:\n",
    "       - Explain how the ideas demonstrate an understanding of the field or subject matter.\n",
    "       - Show how these ideas could contribute to informed decision-making in the relevant domain.\n",
    "\n",
    "    8. Holistic Thinking:\n",
    "       - Explore how the ideas might impact related areas or systems.\n",
    "       - Consider long-term implications and cumulative effects.\n",
    "\n",
    "    9. Problem-Solving Skills:\n",
    "       - Identify potential issues or challenges related to the document's content.\n",
    "       - Develop innovative solutions to address these challenges.\n",
    "\n",
    "    10. Mindfulness and Broader Impact:\n",
    "        - Consider how the ideas might affect various stakeholders or environments.\n",
    "        - Explore potential ethical implications or societal impacts.\n",
    "\n",
    "    11. Summary of Thought Process:\n",
    "        - Provide a concise overview of the analytical reasoning used to generate the ideas.\n",
    "        - Highlight the key factors that led to the most promising ideas.\n",
    "\n",
    "    12. Implications of Conclusions:\n",
    "        - Discuss the potential benefits of implementing the generated ideas.\n",
    "        - Explore how these ideas might influence future developments in the field.\n",
    "\n",
    "    Present your analysis and ideas in a clear, structured format. Focus on generating creative yet logically sound ideas that build upon the document's content in meaningful ways, similar to the dental hygiene example provided.\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an analytical idea generator, capable of producing creative and well-reasoned ideas based on complex document analysis. Your process mirrors the comprehensive thought process used in the dental hygiene example.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def visualize_chunk_relationships(similarity_matrix):\n",
    "    G = nx.Graph()\n",
    "    for i in range(len(similarity_matrix)):\n",
    "        G.add_node(i)\n",
    "        for j in range(i+1, len(similarity_matrix)):\n",
    "            if similarity_matrix[i][j] > 0.5:  # Adjust threshold as needed\n",
    "                G.add_edge(i, j, weight=similarity_matrix[i][j])\n",
    "\n",
    "    pos = nx.spring_layout(G)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    nx.draw(G, pos, with_labels=True, node_color='lightblue',\n",
    "            node_size=1000, font_size=8, font_weight='bold')\n",
    "    edge_weights = nx.get_edge_attributes(G, 'weight')\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_weights)\n",
    "    plt.title(\"Chunk Relationship Network\")\n",
    "    plt.savefig(\"chunk_relationships.png\")\n",
    "    plt.close()\n",
    "\n",
    "def visualize_concept_hierarchy(key_concepts):\n",
    "    all_concepts = [concept for chunk_concepts in key_concepts for concept in chunk_concepts]\n",
    "    unique_concepts = list(set(all_concepts))\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([' '.join(concepts) for concepts in key_concepts])\n",
    "\n",
    "    linkage_matrix = linkage(tfidf_matrix.toarray(), method='ward')\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    dendrogram(linkage_matrix, labels=unique_concepts, leaf_rotation=90, leaf_font_size=8)\n",
    "    plt.title(\"Hierarchy of Key Concepts\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"concept_hierarchy.png\")\n",
    "    plt.close()\n",
    "\n",
    "def main(pdf_path):\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    print(\"Splitting text into chunks...\")\n",
    "    chunks = split_text_into_chunks(text)\n",
    "\n",
    "    print(\"Processing chunks...\")\n",
    "    chunk_embeddings = process_chunks(chunks)\n",
    "\n",
    "    print(\"Analyzing chunk relationships...\")\n",
    "    similarity_matrix = analyze_chunk_relationships(chunk_embeddings)\n",
    "\n",
    "    print(\"Identifying key concepts...\")\n",
    "    key_concepts = identify_key_concepts(chunks)\n",
    "\n",
    "    print(\"Storing data in Neo4j...\")\n",
    "    store_in_neo4j(chunks, similarity_matrix, key_concepts)\n",
    "\n",
    "    print(\"Generating emergent ideas...\")\n",
    "    ideas = generate_emergent_ideas(chunks, similarity_matrix, key_concepts)\n",
    "\n",
    "    print(\"Visualizing chunk relationships...\")\n",
    "    visualize_chunk_relationships(similarity_matrix)\n",
    "\n",
    "    print(\"Visualizing concept hierarchy...\")\n",
    "    visualize_concept_hierarchy(key_concepts)\n",
    "\n",
    "    return chunks, similarity_matrix, key_concepts, ideas\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"/path/to/your/document.pdf\"  # Replace with actual path\n",
    "\n",
    "    try:\n",
    "        chunks, similarity_matrix, key_concepts, ideas = main(pdf_path)\n",
    "\n",
    "        print(\"\\nDocument Processing Complete\")\n",
    "        print(f\"\\nProcessed {len(chunks)} chunks\")\n",
    "        print(\"\\nKey Concepts (first 5 chunks):\")\n",
    "        for i, concepts in enumerate(key_concepts[:5]):\n",
    "            print(f\"Chunk {i}: {', '.join(concepts)}\")\n",
    "\n",
    "        print(\"\\nEmergent Ideas:\")\n",
    "        print(ideas)\n",
    "        print(\"\\nNote: These ideas were generated using a structured analytical approach, mirroring the comprehensive thought process used in the dental hygiene example.\")\n",
    "\n",
    "        print(\"\\nChunk relationship visualization saved as 'chunk_relationships.png'\")\n",
    "        print(\"Concept hierarchy visualization saved as 'concept_hierarchy.png'\")\n",
    "\n",
    "        print(\"\\nData has been stored in Neo4j for further querying and analysis.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(\"Please check your file path and ensure all dependencies are correctly installed.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPwGd1vsaq4io3SzGx6T3fU",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
